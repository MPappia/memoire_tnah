
@incollection{terras_role_2022,
	title = {The role of the library when computers can read: {Critically} adopting {Handwritten} {Text} {Recognition} ({HTR}) technologies to support research},
	shorttitle = {The role of the library when computers can read},
	url = {https://www.research.ed.ac.uk/en/publications/the-role-of-the-library-when-computers-can-read-critically-adopti},
	language = {English},
	urldate = {2024-08-24},
	booktitle = {The {Rise} of {AI}: {Implications} and {Applications} of {Artificial} {Intelligence} in {Academic} {Libraries}},
	publisher = {ACRL - Association of College \& Research Libraries},
	author = {Terras, Melissa},
	month = mar,
	year = {2022},
	pages = {137--148},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/LX4E58TS/Terras - 2022 - The role of the library when computers can read C.pdf:application/pdf},
}

@article{nockels_gooding_ames_terras_2022,
	type = {Article},
	title = {Understanding the application of handwritten text recognition technology in heritage contexts: A systematic review of Transkribus in published research},
	shorttitle = {Understanding the application of HTR in heritage contexts},
	url = {https://doi.org/10.1007/s10502-022-09397-0},
	journal = {Archival Science},
	volume = {22},
	number = {3},
	pages = {367-392},
	author = {Nockels, Joe and Gooding, Paul and Ames, Sarah and Terras, Melissa},
	month = may,
	year = {2022},
	doi = {10.1007/s10502-022-09397-0},
}

@misc{souvay_acceau_2023,
	type = {Billet},
	title = {Accéder au plein texte dans la minute : l’emploi des méthodes {HTR} dans une démarche d’histoire computationnelle},
	shorttitle = {Accéder au plein texte dans la minute},
	url = {https://dhiha.hypotheses.org/3224},
	abstract = {J’ai eu le privilège d’être accueilli en tant que Resident Fellow de février à juillet 2023 à l’Institut historique allemand à Paris dans le cadre de mon projet de thèse, commencé en août 2023...},
	language = {fr-FR},
	urldate = {2024-08-24},
	journal = {Digital Humanities à l'Institut historique allemand},
	author = {Souvay, Hippolyte},
	month = oct,
	year = {2023},
	doi = {10.58079/nkzt},
	note = {ISSN: 2262-581X},
	file = {Snapshot:/Users/mpappia/Zotero/storage/MEGIE274/3224.html:text/html},
}

@misc{bruckmann_numerisation_2012,
	type = {Text},
	title = {La numérisation à la {Bibliothèque} nationale de {France} et les investissements d’avenir},
	url = {https://bbf.enssib.fr/consulter/bbf-2012-04-0049-010},
	abstract = {En partie en réaction aux réalisations de Google en la matière, la Bibliothèque nationale de France (BnF) s’est lancée depuis 2007 dans des opérations de numérisation de masse, aux coûts très élevés. Dans le cadre de l’emprunt national aux investissements d’avenir, l’État a créé pour la mise en œuvre de la part de ces programmes consacrés au développement de l’économie numérique un Fonds national pour la société numérique (FSN), dont le mode de fonctionnement, dit de « l’investisseur avisé », inclut une gestion comparable à celle d’une entreprise privée. Pour financer ses opérations de numérisation, la BnF a lancé un appel à partenariats et mis en place une filiale spécifique, « BnF-Partenariats ».},
	language = {fr},
	urldate = {2024-08-24},
	author = {Bruckmann, Denis and Thouny, Nathalie},
	month = jan,
	year = {2012},
	file = {Snapshot:/Users/mpappia/Zotero/storage/T34XYRPP/bbf-2012-04-0049-010.html:text/html},
}

@article{engel_numerique_2022,
	title = {Le numérique à la {BnF} : un objectif global},
	volume = {2022},
	issn = {1148-7941},
	shorttitle = {Le numérique à la {BnF}},
	url = {https://shs.cairn.info/article/RINDU1_221_0050?lang=fr&tab=resume},
	doi = {10.3917/rindu1.221.0050},
	language = {fr},
	number = {1},
	urldate = {2024-08-24},
	journal = {Annales des Mines - Réalités industrielles},
	author = {Engel, Laurence},
	year = {2022},
	note = {Publisher: Institut Mines-Télécom},
	pages = {50--52},
}

@misc{racine_schema_2009,
	author = {Bruno Racine},
	title = {Schéma numérique des bibliothèques},
	year = {2009},
	month = dec,
	language = {fr},
	note = {Rapport élaboré dans le cadre du Conseil du Livre, Paris, Ministère de la Culture et de la Communication},
}

@article{bermes_numerique_2020,
	author = {Emmanuelle Bermès},
	title = {Le numérique en bibliothèque : naissance d’un patrimoine : l’exemple de la {Bibliothèque} nationale de {France} (1997-2019)},
	journal = {École nationale des chartes},
	year = {2020},
	pages = {26--34},
	language = {fr},
	note = {Le numérique en bibliothèque},
}

@article{racine_bibliotheque_2011,
	title = {La {Bibliothèque} nationale devant le numérique},
	volume = {415},
	issn = {0014-1941},
	url = {https://shs.cairn.info/article/ETU_4156_0605?lang=fr&tab=texte-integral},
	doi = {10.3917/etu.4156.0605},
	language = {fr},
	number = {12},
	urldate = {2024-08-24},
	journal = {Études},
	author = {Racine, Bruno},
	year = {2011},
	note = {Publisher: S.E.R.},
	pages = {605--616},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/X26BVYUV/Racine - 2011 - La Bibliothèque nationale devant le numérique.pdf:application/pdf},
}

@article{martin_concept_2010,
	author = {Frédéric Martin and Emmanuelle Bermès},
	title = {Le concept de collection numérique},
	journal = {Bulletin des bibliothèques de France (bbf)},
	volume = {55},
	number = {3},
	pages = {13--17},
	year = {2010},
	language = {fr},
}

@misc{leroy-terquem_milles_2023,
	author = {Mélanie Leroy-Terquem},
	title = {Les milles et une vies des collections numérisées},
	year = {2023},
	month = jan,
	language = {fr},
	note = {Chroniques, no 96 (janvier-mars 2023), p. 1‑6},
	url = {https://www.bnf.fr/fr/les-milles-et-une-vies-des-collections-numerisees},
}

@inproceedings{byrum_iso_1999,
	author = {John D. Byrum},
	title = {{ISO} 639-1 and {ISO} 639-2: {International} {Standards} for {Language} {Codes}. {ISO} 15924: {International} {Standard} for names of scripts},
	booktitle = {IFLA Council and General Conference. Conference Programme and Proceedings},
	eventtitle = {65e, Bangkok, Thaïlande, 20-28 août 1999},
	publisher = {Library of Congress},
	year = {1999},
	pages = {1--6},
	language = {en},
}

@misc{kreuzberger_mlops_2023,
	author = {Dominik Kreuzberger and Niklas Kühl and Sebastian Hirschl},
	title = {Machine {Learning} {Operations} ({MLOps}): {Overview}, {Definition}, and {Architecture}},
	howpublished = {IBM},
	year = {2023},
	pages = {5},
	url = {https://ieeexplore.ieee.org/abstract/document/10081336},
	urldate = {2024-08-24},
	language = {en},
}

@misc{bnf_printemps_2024,
	address = {Paris},
	type = {Présentation {PowerPoint}},
	title = {Printemps de l'{OCR}.},
	author = {BNF, Bibliothèque Nationale de France},
	month = apr,
	year = {2024},
}

@article{carlin_bnf_2021,
	title = {Le {BnF} {DataLab}, un service aux chercheurs en humanités numériques},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2736-2337},
	url = {https://journals.openedition.org/revuehn/2684},
	doi = {10.4000/revuehn.2684},
	abstract = {Depuis une dizaine d’années, l’augmentation massive des collections numériques de la Bibliothèque nationale de France (BNF) – Gallica, archives de l’Internet, métadonnées, etc. – ouvre de nouvelles pistes de recherche, faisant émerger des problématiques d’exploitation des données patrimoniales par les chercheurs. Pour répondre à leurs usages et besoins, la BNF met en place un nouveau service, le BnF DataLab, dont l’objectif est d’accompagner les équipes de recherche pour constituer et traiter des corpus numériques. Ce lieu, conçu pour le travail individuel et collaboratif, a ouvert ses portes en octobre 2021.},
	language = {fr},
	number = {4},
	urldate = {2024-08-24},
	journal = {Humanités numériques},
	author = {Carlin, Marie and Laborderie, Arnaud},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Humanistica},
	keywords = {archives, bibliothèque numérique, collection, numérisation, sciences de l’information et de la communication, Web},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/GFENZSFK/Carlin et Laborderie - 2021 - Le BnF DataLab, un service aux chercheurs en human.pdf:application/pdf},
}

@article{solbakk_implementation_nodate,
	title = {Implementation of digital deposit at the {National} {Library} of {Norway}},
	abstract = {The paper will outline the strategies at the National Library of Norway for capturing digital born publications directly from the publishers, even though the publication itself is on paper or other physical media. Also the actual implementation of these strategies and lessons learned in the process will be presented. This includes both the dialog with the publishers, the technical solutions for digital capture, and the processes for handling digital publications within the library. The paper will focus on a concrete implementation for newspapers. However, the principles are general.},
	language = {en},
	author = {Solbakk, Svein Arne},
	file = {Solbakk - Implementation of digital deposit at the National .pdf:/Users/mpappia/Zotero/storage/V7QS6ADI/Solbakk - Implementation of digital deposit at the National .pdf:application/pdf},
}

@article{vezina_overview_2020,
	title = {An {Overview} of the {BALSAC} {Population} {Database}. {Past} {Developments}, {Current} {State} and {Future} {Prospects}},
	volume = {9},
	copyright = {Copyright (c) 2021 Hélène Vézina, Jean-Sébastien Bournival},
	issn = {2352-6343},
	url = {https://hlcs.nl/article/view/9299},
	doi = {10.51964/hlcs9299},
	abstract = {The BALSAC database, developed since 1971, contains data on the Quebec population from the beginnings of European settlement in the 17th century to the contemporary period. Today, BALSAC is a major research infrastructure used by researchers from Quebec and elsewhere, both in the social sciences and in the biomedical sciences. This paper presents the evolution and current state of the database and offers a perspective on forthcoming developments. BALSAC contains marriage certificates until 1965. Coverage is complete for Catholic records (80 to 100\% of the population depending on the region and the period) and partial for the other denominations. Birth and death certificates from all Catholic parishes have been integrated for the period 1800–1849 and work in underway for 1850–1916. All the records entered in BALSAC are subject to a linkage process which, ultimately, allows the automatic reconstitution of genealogical links and family relationships. The basic principle has remained the same since the beginning, namely to match individuals based on the nominative information contained in the sources. The changes made in recent years and the resulting gains are mostly related to IT advances which now offer more flexibility and increased performance. Future perspectives rest on the diversification of the sources of population data entered or connected to the database and, as a corollary, by continuous optimization of data processing and linkage procedures. In the era of 'big data', BALSAC is gradually moving from a historical population database to a multifaceted infrastructure for interdisciplinary research on the Quebec population.},
	language = {en},
	urldate = {2024-08-24},
	journal = {Historical Life Course Studies},
	author = {Vézina, Hélène and Bournival, Jean-Sébastien},
	month = aug,
	year = {2020},
	keywords = {Family reconstitution, Population database, Quebec population, Record linkage, Vital records},
	pages = {114--129},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/YGLHRRV2/Vézina et Bournival - 2020 - An Overview of the BALSAC Population Database. Pas.pdf:application/pdf},
}

@article{malesev_need_2022,
	title = {The {Need} for {Continuous} {Professional} {Development} of {Librarians} - a {Benefit} to the {Individual} and {Society}},
	volume = {27},
	issn = {1512-5033, 2303-8888},
	url = {https://hrcak.srce.hr/clanak/415441},
	doi = {10.37083/bosn.2022.27.35},
	abstract = {U radu se govori o potrebi stalnog stručnog usavršavanja u svim sferama ljudskog rada i delovanja. Akcenat je na edukaciji zaposlenih u ustanovama kulture pa samim tim i u bibliotekama. Na kraju rada opisan je projekat pod nazivom Transkribus, koji i...},
	language = {sr},
	number = {27},
	urldate = {2024-08-24},
	journal = {Bosniaca : časopis Nacionalne i univerzitetske biblioteke Bosne i Hercegovine},
	author = {Malešev, Tamara and Topalov, Olivеra and Radovanović, Slađana},
	month = dec,
	year = {2022},
	note = {Publisher: Nacionalna i univerzitetska biblioteka Bosna i Hercegovina},
	pages = {35--48},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/8MX5KN4U/Malešev et al. - 2022 - The Need for Continuous Professional Development o.pdf:application/pdf},
}

@misc{bui_recognizing_2023,
	title = {Recognizing {Automatically} {Dunhuang} {Chinese} {Manuscripts} ({Read} {Chinese}), {Appel} à projets {BnF} {DataLab}},
	publisher = {Laboratoire AOROC UMR 8546 CNRS-Université PSL, ENS \& EPHE},
	author = {Bui, Marc and al.},
	year = {2023},
}

@inproceedings{liu_swin_2021,
	address = {Montreal, QC, Canada},
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	shorttitle = {Swin {Transformer}},
	url = {https://ieeexplore.ieee.org/document/9710580/},
	doi = {10.1109/ICCV48922.2021.00986},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the ﬂexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classiﬁcation (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneﬁcial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer.},
	language = {en},
	urldate = {2024-08-24},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = oct,
	year = {2021},
	pages = {9992--10002},
	file = {Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:/Users/mpappia/Zotero/storage/B3ZRX3U6/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf},
}

@misc{coulie_valorisation_2022,
	title = {Valorisation de documents manuscrits arméniens conservés à la {BnF} : {Recueils} de correspondances. {Textes} historiographiques du fonds {Dulaurier}, {Appel} à projets {BnF} {DataLab}},
	publisher = {UCLouvain - Centre d’études orientales, Institut orientaliste de Louvain},
	author = {Coulie, Bernard and al.},
	year = {2022},
}

@article{kindt_du_2021,
	title = {Du texte à l’ index. {L}’ étiquetage lexical du {De} {Septem} {Orbis} {Spectaculis} de {Philon} le {Paradoxographe} : méthode et finalité},
	copyright = {free},
	shorttitle = {Du texte à l’ index. {L}’ étiquetage lexical du {De} {Septem} {Orbis} {Spectaculis} de {Philon} le {Paradoxographe}},
	url = {https://www.persee.fr/doc/ista_0000-0000_2021_act_1521_1_3881},
	abstract = {Cet article décrit les différents étapes nécessaires pour traiter le vocabulaire d’un texte en grec ancien afin d’en tirer des index et des concordances lemmatisées, monolingues (traitement d’un texte seul) ou bilingues (traitement d’un texte et de sa traduction). Le texte grec du De Septem Orbis Spectaculis et sa traduction française servent de fil rouge à cette démonstration. Les outils de traitement mis en oeuvre sont développés dans le cadre du projet GREgORI, mené à l’Institut orientaliste de l’Université catholique de Louvain en partenariat avec le Centre de Traitement Automatique du Langage (CENTAL), laboratoire spécialisé dans l’étude du traitement informatique des langues, appartenant à la même université. L’article rappelle les objectifs du projet, décrit les outils de lemmatisation utilisés et procure aux lecteurs des exemples de concordances et d’index, en version monolingue ou bilingue (grec-français). Ces travaux ouvrent la voie pour des développements ultérieurs utiles à l’étude du vocabulaire des textes grecs historiques et patristiques et de leurs versions dans différentes langues de l’Orient chrétien.},
	language = {fre},
	urldate = {2024-08-24},
	author = {Kindt, Bastien},
	year = {2021},
	note = {Publisher: Persée - Portail des revues scientifiques en SHS},
}

@misc{clerice_htromance_2022,
	title = {{HTRomance} : {Traitement} automatique des manuscrits en langues romanes, {Appel} à projets {BnF}},
	publisher = {Centre Jean Mabillon, École nationale des chartes \& INRIA},
	author = {Clérice, Thibault and al.},
	year = {2022},
}

@article{clerice_you_2023,
	title = {You {Actually} {Look} {Twice} {At} it ({YALTAi}): using an object detection approach instead of region segmentation within the {Kraken} engine},
	volume = {Historical Documents and...},
	issn = {2416-5999},
	shorttitle = {You {Actually} {Look} {Twice} {At} it ({YALTAi})},
	url = {https://jdmdh.episciences.org/9806},
	doi = {10.46298/jdmdh.9806},
	abstract = {Layout Analysis (the identification of zones and their classification) and line segmentation are the first steps in Optical Character Recognition and similar tasks. The ability of identifying the main body of text from marginal text or running titles makes the difference between extracting the full text of a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competitions on historical documents (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We suggest that transitioning the task from pixel classification-based polygonization to object detection using isothetic rectangles might improve results in terms of speed and accuracy. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the latter severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of Kraken 4.1.},
	language = {en},
	urldate = {2024-08-24},
	journal = {Journal of Data Mining \& Digital Humanities},
	author = {Clérice, Thibault},
	month = dec,
	year = {2023},
	pages = {9806},
	file = {Clérice - 2023 - You Actually Look Twice At it (YALTAi) using an o.pdf:/Users/mpappia/Zotero/storage/6VVWTMSH/Clérice - 2023 - You Actually Look Twice At it (YALTAi) using an o.pdf:application/pdf},
}

@article{chague_escriptorium_2022,
	title = {{eScriptorium} : une application libre pour la transcription automatique des manuscrits},
	issn = {2108-7016, 1269-0589},
	shorttitle = {{eScriptorium}},
	url = {https://publications-prairial.fr/arabesques/index.php?id=3100},
	doi = {10.35562/arabesques.3100},
	language = {fr},
	number = {107},
	urldate = {2024-08-24},
	journal = {Arabesques},
	author = {Chagué, Alix},
	month = oct,
	year = {2022},
	pages = {25},
	file = {Chagué - 2022 - eScriptorium  une application libre pour la trans.pdf:/Users/mpappia/Zotero/storage/5BGDPGR9/Chagué - 2022 - eScriptorium  une application libre pour la trans.pdf:application/pdf},
}

@phdthesis{de_lavenne_de_la_montoise_oai-pmh_2020,
	address = {Villeurbanne},
	type = {Mémoire pour le diplôme de conservateur de bibliothèque},
	title = {{OAI}-{PMH} à « l’heure du web sémantique » : bilans et perspectives},
	school = {ENSSIB},
	author = {de Lavenne de la Montoise, Vincent},
	year = {2020},
}

@inproceedings{bermes_approche_2010,
	author = {Emmanuelle Bermès and Louise Fauduet and Sébastien Peyrard},
	title = {Une approche orientée données pour la préservation du numérique : le projet {SPAR}},
	booktitle = {WORLD LIBRARY AND INFORMATION CONGRESS: 76th IFLA GENERAL CONFERENCE AND ASSEMBLY},
	address = {Gothenburg, Suède},
	year = {2010},
	month = aug,
	pages = {1--11},
	note = {Présenté du 10 au 15 août 2010},
}

@inproceedings{tafti_ocr_2016,
	address = {Cham},
	title = {{OCR} as a {Service}: {An} {Experimental} {Evaluation} of {Google} {Docs} {OCR}, {Tesseract}, {ABBYY} {FineReader}, and {Transym}},
	isbn = {978-3-319-50835-1},
	shorttitle = {{OCR} as a {Service}},
	doi = {10.1007/978-3-319-50835-1_66},
	abstract = {Optical character recognition (OCR) as a classic machine learning challenge has been a longstanding topic in a variety of applications in healthcare, education, insurance, and legal industries to convert different types of electronic documents, such as scanned documents, digital images, and PDF files into fully editable and searchable text data. The rapid generation of digital images on a daily basis prioritizes OCR as an imperative and foundational tool for data analysis. With the help of OCR systems, we have been able to save a reasonable amount of effort in creating, processing, and saving electronic documents, adapting them to different purposes. A set of different OCR platforms are now available which, aside from lending theoretical contributions to other practical fields, have demonstrated successful applications in real-world problems. In this work, several qualitative and quantitative experimental evaluations have been performed using four well-know OCR services, including Google Docs OCR, Tesseract, ABBYY FineReader, and Transym. We analyze the accuracy and reliability of the OCR packages employing a dataset including 1227 images from 15 different categories. Furthermore, we review the state-of-the-art OCR applications in healtcare informatics. The present evaluation is expected to advance OCR research, providing new insights and consideration to the research area, and assist researchers to determine which service is ideal for optical character recognition in an accurate and efficient manner.},
	language = {en},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Tafti, Ahmad P. and Baghaie, Ahmadreza and Assefi, Mehdi and Arabnia, Hamid R. and Yu, Zeyun and Peissig, Peggy},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Porikli, Fatih and Skaff, Sandra and Entezari, Alireza and Min, Jianyuan and Iwai, Daisuke and Sadagic, Amela and Scheidegger, Carlos and Isenberg, Tobias},
	year = {2016},
	pages = {735--746},
}

@article{olson_digitization_2021,
	title = {Digitization {Decisions}: {Comparing} {OCR} {Software} for {Librarian} and {Archivist} {Use}},
	issn = {1940-5758},
	shorttitle = {Digitization {Decisions}},
	url = {https://journal.code4lib.org/articles/16132?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+c4lj+%28The+Code4Lib+Journal%29},
	abstract = {This paper is intended to help librarians and archivists who are involved in digitization work choose optical character recognition (OCR) software. The paper provides an introduction to OCR software for digitization projects, and shares the method we developed for easily evaluating the effectiveness of OCR software on resources we are digitizing., We tested three major OCR programs (Adobe Acrobat, ABBYY FineReader, Tesseract) for accuracy on three different digitized texts from our archives and special collections at the University of Western Ontario. Our test was divided into two parts: a word accuracy test (to determine how searchable the final documents were), and a test with a screen reader (to determine how accessible the final documents were). We share our findings from the tests and make recommendations for OCR work on digitized documents from archives and special collections.},
	number = {52},
	urldate = {2024-08-24},
	journal = {The Code4Lib Journal},
	author = {Olson, Leanne and Berry, Veronica},
	month = sep,
	year = {2021},
	file = {Code4Lib Journal Snapshot:/Users/mpappia/Zotero/storage/DMQIQBKR/16132.html:text/html},
}

@inproceedings{belaid_xml_2007,
	title = {{XML} {Data} {Representation} in {Document} {Image} {Analysis}},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/4378679},
	doi = {10.1109/ICDAR.2007.4378679},
	abstract = {This paper presents the XML-based formats ALTO, TEI, METS used for Digital Libraries and their interest for data representation in a Document Image Analysis and Recog- nition (DIAR) process. In the first part we briefly present these formats with focus on their adequacy for structural representation and modeling of DIAR data. The second part shows how these formats can be used in a reverse engineer- ing process. Their implementation as a data representation framework will be shown.},
	urldate = {2024-08-24},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Belaid, A. and Rangoni, Y. and Falk, I.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Encoding, Guidelines, Image analysis, Image recognition, Optical character recognition software, Reverse engineering, Software libraries, Text analysis, Text recognition, XML},
	pages = {78--82},
	file = {IEEE Xplore Abstract Record:/Users/mpappia/Zotero/storage/KYGX8598/4378679.html:text/html},
}

@article{cretin_transcription_2023,
	title = {La transcription automatique d'écritures manuscrites: premiers pas à la {BnF}},
	language = {fr},
	author = {Cretin, Sebastien},
	year = {2023},
	file = {Cretin - 2023 - La transcription automatique d'écritures manuscrit.pdf:/Users/mpappia/Zotero/storage/BD8CTGVJ/Cretin - 2023 - La transcription automatique d'écritures manuscrit.pdf:application/pdf},
}

@incollection{goos_dublin_2002,
	address = {Berlin, Heidelberg},
	title = {Dublin {Core}: {Process} and {Principles}},
	volume = {2555},
	isbn = {978-3-540-00261-1 978-3-540-36227-2},
	shorttitle = {Dublin {Core}},
	url = {http://link.springer.com/10.1007/3-540-36227-4_3},
	abstract = {The Dublin Core metadata element set has been widely adopted by cultural and scientific institutions, libraries, governments, and businesses to describe resources for discovery on the Internet. This paper provides an overview of its history and underlying principles and describes the activities of Dublin Core Metadata Initiative as an organization.},
	language = {en},
	urldate = {2024-08-24},
	booktitle = {Digital {Libraries}: {People}, {Knowledge}, and {Technology}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sugimoto, Shigeo and Baker, Thomas and Weibel, Stuart L.},
	editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Lim, Ee- Peng and Foo, Schubert and Khoo, Chris and Chen, Hsinchun and Fox, Edward and Urs, Shalini and Costantino, Thanos},
	year = {2002},
	doi = {10.1007/3-540-36227-4_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {25--35},
	file = {Sugimoto et al. - 2002 - Dublin Core Process and Principles.pdf:/Users/mpappia/Zotero/storage/I6X4J5D9/Sugimoto et al. - 2002 - Dublin Core Process and Principles.pdf:application/pdf},
}

@misc{manrique-gomez_historical_2024,
	title = {Historical {Ink}: 19th {Century} {Latin} {American} {Spanish} {Newspaper} {Corpus} with {LLM} {OCR} {Correction}},
	shorttitle = {Historical {Ink}},
	url = {http://arxiv.org/abs/2407.12838},
	abstract = {This paper presents two significant contributions: first, a novel dataset of 19th-century Latin American press texts, which addresses the lack of specialized corpora for historical and linguistic analysis in this region. Second, it introduces a framework for OCR error correction and linguistic surface form detection in digitized corpora, utilizing a Large Language Model. This framework is adaptable to various contexts and, in this paper, is specifically applied to the newly created dataset.},
	language = {en},
	urldate = {2024-08-24},
	publisher = {arXiv},
	author = {Manrique-Gómez, Laura and Montes, Tony and Manrique, Rubén},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12838 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Digital Libraries, I.2.7},
	file = {Manrique-Gómez et al. - 2024 - Historical Ink 19th Century Latin American Spanis.pdf:/Users/mpappia/Zotero/storage/I799UGTE/Manrique-Gómez et al. - 2024 - Historical Ink 19th Century Latin American Spanis.pdf:application/pdf},
}

@inproceedings{kahle_transkribus_2017,
	address = {Kyoto},
	title = {Transkribus - {A} {Service} {Platform} for {Transcription}, {Recognition} and {Retrieval} of {Historical} {Documents}},
	isbn = {978-1-5386-3586-5},
	url = {http://ieeexplore.ieee.org/document/8270253/},
	doi = {10.1109/ICDAR.2017.307},
	urldate = {2024-08-24},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {IEEE},
	author = {Kahle, Philip and Colutto, Sebastian and Hackl, Gunter and Muhlberger, Gunter},
	month = nov,
	year = {2017},
	pages = {19--24},
}

@article{nockels_implications_2024,
	title = {The implications of handwritten text recognition for accessing the past at scale},
	volume = {80},
	issn = {0022-0418},
	url = {https://doi.org/10.1108/JD-09-2023-0183},
	doi = {10.1108/JD-09-2023-0183},
	abstract = {Purpose This paper focuses on image-to-text manuscript processing through Handwritten Text Recognition (HTR), a Machine Learning (ML) approach enabled by Artificial Intelligence (AI). With HTR now achieving high levels of accuracy, we consider its potential impact on our near-future information environment and knowledge of the past.Design/methodology/approach In undertaking a more constructivist analysis, we identified gaps in the current literature through a Grounded Theory Method (GTM). This guided an iterative process of concept mapping through writing sprints in workshop settings. We identified, explored and confirmed themes through group discussion and a further interrogation of relevant literature, until reaching saturation.Findings Catalogued as part of our GTM, 120 published texts underpin this paper. We found that HTR facilitates accurate transcription and dataset cleaning, while facilitating access to a variety of historical material. HTR contributes to a virtuous cycle of dataset production and can inform the development of online cataloguing. However, current limitations include dependency on digitisation pipelines, potential archival history omission and entrenchment of bias. We also cite near-future HTR considerations. These include encouraging open access, integrating advanced AI processes and metadata extraction; legal and moral issues surrounding copyright and data ethics; crediting individuals’ transcription contributions and HTR’s environmental costs.Originality/value Our research produces a set of best practice recommendations for researchers, data providers and memory institutions, surrounding HTR use. This forms an initial, though not comprehensive, blueprint for directing future HTR research. In pursuing this, the narrative that HTR’s speed and efficiency will simply transform scholarship in archives is deconstructed.},
	number = {7},
	urldate = {2024-08-24},
	journal = {Journal of Documentation},
	author = {Nockels, Joseph and Gooding, Paul and Terras, Melissa},
	month = jan,
	year = {2024},
	note = {Publisher: Emerald Publishing Limited},
	pages = {148--167},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/ED5FDUGY/Nockels et al. - 2024 - The implications of handwritten text recognition f.pdf:application/pdf},
}

@inproceedings{chague_cremma_2021,
	author = {Alix Chagué},
	title = {{CREMMA}: {Une} infrastructure mutualisée pour la reconnaissance d'écritures manuscrites et la patrimonialisation numérique},
	booktitle = {Sciences du patrimoine - sciences du texte. Confrontation des méthodes},
	address = {École nationale des chartes, Paris, France},
	year = {2021},
	month = may,
	url = {https://hal.archives-ouvertes.fr/hal-03541887},
	language = {fr},
}

@inproceedings{chague_manu_2023,
	title = {Manu {McFrench}, from zero to hero: impact of using a generic handwriting recognition model for smaller datasets},
	shorttitle = {Manu {McFrench}, from zero to hero},
	url = {https://inria.hal.science/hal-04094241},
	abstract = {Long paper presentation for ADHO's annual conference on Digital Humanities (2023), discussing the importance of using generic transcription models for HTR and how to create them. We use the case of the CREMMA datasets and the Manu McFrench models as an example.},
	language = {en},
	urldate = {2024-08-24},
	author = {Chagué, Alix and Clérice, Thibault and Norindr, Jade and Humeau, Maxime and Davoury, Baudoin and Kote, Elsa Van and Mazoue, Anaïs and Faure, Margaux and Doat, Soline},
	month = jul,
	year = {2023},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/KQLETEJY/Chagué et al. - 2023 - Manu McFrench, from zero to hero impact of using .pdf:application/pdf},
}

@article{pinche_generic_2023,
	title = {Generic {HTR} {Models} for {Medieval} {Manuscripts}. {The} {CREMMALab} {Project}},
	volume = {Historical Documents and automatic text recognition},
	issn = {2416-5999},
	url = {https://jdmdh.episciences.org/11592},
	doi = {10.46298/jdmdh.10252},
	abstract = {In the Humanities, the emergence of digital methods has opened up research questions to quantitative analysis. This is why HTR technology is increasingly involved in humanities research projects following precursors such as the Himanis project. However, many research teams have limited resources, either financially or in terms of their expertise in artificial intelligence. It may therefore be difficult to integrate handwritten text recognition into their project pipeline if they need to train a model or to create data from scratch. The goal here is not to explain how to build or improve a new HTR engine, nor to find a way to automatically align a preexisting corpus with an image to quickly create ground truths for training. This paper aims to help humanists easily develop an HTR model for medieval manuscripts, create and gather training data by knowing the issues underlying their choices. The objective is also to show the importance of the constitution of consistent data as a prerequisite to allow their gathering and to train efficient HTR models. We will present an overview of our work and experiment in the CREMMALab project (2021-2022), showing first how we ensure the consistency of the data and then how we have developed a generic model for medieval French manuscripts from the 13 th to the 15 th century, ready to be shared (more than 94\% accuracy) and/or fine-tuned by other projects.},
	language = {en},
	urldate = {2024-08-24},
	journal = {Journal of Data Mining \& Digital Humanities},
	author = {Pinche, Ariane},
	month = oct,
	year = {2023},
	note = {Publisher: Episciences.org},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/ZUPS247L/Pinche - 2023 - Generic HTR Models for Medieval Manuscripts. The C.pdf:application/pdf},
}

@inproceedings{pinche_images_2022,
	author = {Ariane Pinche},
	title = {Des images au texte : comment apprendre à des ordinateurs à lire des manuscrits médiévaux ?},
	booktitle = {Séminaire « De l’écrit à l’écran. Objets numériques et matérialité des sources écrites »},
	year = {2022},
	language = {fr},
}

@unpublished{pinche_htr_2022,
	title = {{HTR} {Models} and genericity for {Medieval} {Manuscripts}},
	url = {https://hal.science/hal-03736532},
	abstract = {Within the infrastructure of the CREMMA project (Consortium for Handwriting Recognition of Ancient Materials) supported by the DIM (research funded by the Île-de-France Region) MAP (Ancient and Heritage Materials), the CREMMALab 1 project combines research questions, creation and release of data from medieval French literary manuscripts for HTR. The objective of the CREMMALab project is to propose open training data and HTR models for medieval documents. All data and models produced by the project are already available in the CREMMA Medieval repository (Pinche 2022) on HTR-united catalogue (Chagué, Clérice, and Chiffoleau, 2021). In accordance with this objective, the project implements transcription protocols to optimise the training of HTR models and to produce homogeneous and shareable data and models.},
	urldate = {2024-08-24},
	author = {Pinche, Ariane},
	month = jul,
	year = {2022},
	annote = {working paper or preprint},
	file = {HAL PDF Full Text:/Users/mpappia/Zotero/storage/SFK9D7BB/Pinche - 2022 - HTR Models and genericity for Medieval Manuscripts.pdf:application/pdf},
}

@article{hodel_general_2021,
	title = {General {Models} for {Handwritten} {Text} {Recognition}: {Feasibility} and {State}-of-the {Art}. {German} {Kurrent} as an {Example}},
	volume = {7},
	copyright = {info:eu-repo/semantics/openAccess},
	issn = {2059-481X},
	shorttitle = {General {Models} for {Handwritten} {Text} {Recognition}},
	url = {https://boris.unibe.ch/157474/},
	abstract = {Existing text recognition engines enables to train general models to recognize not only one specific hand but a multitude of historical hands within a particular script, and from a rather large time period (more than 100 years). This paper compares different text recognition engines and their performance on a test set independent of the training and validation sets. We argue that both, test set and ground truth, should be made available by researchers as part of a shared task to allow for the comparison of engines. This will give insight into the range of possible options for institutions in need of recognition models. As a test set, we provide a data set consisting of 2,426 lines which have been randomly selected from meeting minutes of the Swiss Federal Council from 1848 to 1903. To our knowledge, neither the aforementioned text lines, which we take as ground truth, nor the multitude of different hands within this corpus have ever been used to train handwritten text recognition models. In addition, the data set used is perfect for making comparisons involving recognition engines and large training sets due to its variability and the time frame it spans. Consequently, this paper argues that both the tested engines, HTR+ and PyLaia, can handle large training sets. The resulting models have yielded very good results on a test set consisting of unknown but stylistically similar hands.},
	language = {eng},
	number = {13},
	urldate = {2024-08-24},
	journal = {Journal of open humanities data},
	author = {Hodel, Tobias and Schoch, David and Schneider, Christa and Purcell, Jake},
	collaborator = {Hodel, Tobias Mathias and Schoch, David Selim and Schneider, Christa and Purcell, Jake},
	month = jul,
	year = {2021},
	note = {Num Pages: 10
Number: 13
Publisher: Ubiquity Press},
	pages = {1--10},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/3UEJ6J5K/Hodel et al. - 2021 - General Models for Handwritten Text Recognition F.pdf:application/pdf;Snapshot:/Users/mpappia/Zotero/storage/I4PMGBNK/157474.html:text/html},
}

@article{li_trocr_2023,
	title = {{TrOCR}: {Transformer}-{Based} {Optical} {Character} {Recognition} with {Pre}-trained {Models}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{TrOCR}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26538},
	doi = {10.1609/aaai.v37i11.26538},
	abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr.},
	language = {en},
	number = {11},
	urldate = {2024-08-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
	month = jun,
	year = {2023},
	note = {Number: 11},
	keywords = {CV: Language and Vision},
	pages = {13094--13102},
	file = {Full Text PDF:/Users/mpappia/Zotero/storage/MXNLPE54/Li et al. - 2023 - TrOCR Transformer-Based Optical Character Recogni.pdf:application/pdf},
}

@article{riikka_handwritten_nodate,
	title = {Handwritten {Text} {Recognition} ({HTR}) model for historical documents from 17th to 20th centuries – {Using} {TrOCR}},
	abstract = {Handwritten historical documents remain an important part of research materials in different fields even today. When historical documents are digitized, modern text analysis methods can be applied to make them easier to use and analyse. However, texts must first be recognized from images containing text and converted into machine-readable form, and for a long time this has not been possible for handwritten texts. Without the advancements in (open source) machine learning methods and computational power in recent years, handwritten text recognition on a larger scale would not have been possible (Muehlberger et al. 2019). In a research project conducted at the National Archives of Finland, we utilize the pretrained Transformer-based Optical Character Recognition (TrOCR) model developed by Microsoft (Li et al. 2022). It combines an image Transformer encoder and a text Transformer decoder for optical character recognition, replacing traditional CNN- and RNN-based approaches and eliminating the need for additional language models for post-processing accuracy. TrOCR is pre-trained on synthetic data and finetuned on human-labeled datasets, demonstrating superior performance on both printed and handwritten text recognition tasks. We aim to compare the performance of various HTR models developed specifically for the handwriting styles of individual centuries against a super model trained on a comprehensive dataset from the 1600s to 1900s. The goal is to train HTR models to perform with sufficient accuracy on documents in both Finnish and Swedish languages. As an important part of the strategy of the National Archives of Finland high-performing HTR model development can make handwritten historical documents more accessible and easier to use as source materials in many research fields (Lahtinen \& Katajisto 2020, Paju et al. 2020). This research has access to 26800 pages of annotated data. Annotation here refers to the transcription of texts and the marking lines around text lines. On average, one page consists of 30 lines of text. The data is randomly divided into training, validation, and test datasets and weighted across different centuries and languages (Swedish and Finnish) to ensure a sufficiently representative samples from each century and both languages. The training dataset is used for training the HTR model, while the validation set is automatically used by TrOCR for model validation to identify the best model configuration. The test dataset is used to compare different models against each other. Different models are compared, and model accuracy is evaluated using the Character Error Rate (CER) value.},
	language = {en},
	author = {Riikka, Marttila and Sanna, Joska and Mikko, Lipsanen and Atte, Föhr and Ilkka, Jokipii},
	file = {22_Handwritten_Text_Recognitio.pdf:/Users/mpappia/Downloads/22_Handwritten_Text_Recognitio.pdf:application/pdf},
}

@article{lefranc_arletta_2024,
	title = {{ARletta}. {Open}-{Source} {Handwritten} {Text} {Recognition} {Models} for {Historic} {Dutch}},
	volume = {10},
	issn = {2059-481X},
	url = {http://openhumanitiesdata.metajnl.com/articles/10.5334/johd.225/},
	doi = {10.5334/johd.225},
	abstract = {We release ARletta, a series of open-source models for the automated transcription of historic Dutch-language handwritten sources, which has remained a desideratum in the scholarly community until now. All models presented were trained on publicly available data using the open-source kraken engine. Our endeavor focuses on the digitization of a large-scale collection of local police reports (1876–1945). Additionally, we include a supermodel trained on the union of other Dutch-language datasets (extending back to the 17th century) which we hope will be useful as a foundational model for future projects. Our results demonstrate performance that is competitive with proprietary software solutions.},
	language = {en},
	urldate = {2024-08-24},
	journal = {Journal of Open Humanities Data},
	author = {Lefranc, Lith and Van Damme, Ilja and Clérice, Thibault and Kestemont, Mike},
	month = jul,
	year = {2024},
	pages = {43},
	file = {668fd009ef866.pdf:/Users/mpappia/Downloads/668fd009ef866.pdf:application/pdf},
}

@article{vidal-gorene_reconnaissance_2023,
	title = {La reconnaissance automatique d'écriture à l'épreuve des langues peu dotées},
	copyright = {https://creativecommons.org/licenses/by/4.0/deed.fr},
	issn = {2631-9462},
	url = {https://programminghistorian.org/fr/lecons/transcription-automatisee-graphies-non-latines},
	doi = {10.46430/phfr0023},
	abstract = {Ce tutoriel a pour but de décrire les bonnes pratiques pour la création d'ensembles de données et la spécialisation des modèles en fonction d'un projet HTR (*Handwritten Text Recognition*) ou OCR (*Optical Character Recognition*) sur des documents qui n'utilisent pas l'alphabet latin et donc pour lesquels il n'existe pas ou très peu de données d'entraînement déjà disponibles. Le tutoriel a ainsi pour but de montrer des approches de *minimal computing* (ou d'investissement technique minimal) pour l'analyse de collections numériques à grande échelle pour des langues peu dotées. Notre tutoriel se concentrera sur un exemple en grec ancien, puis proposera une ouverture sur le traitement d'écritures arabes maghrébines manuscrites.},
	language = {fr},
	number = {5},
	urldate = {2024-08-24},
	journal = {Programming Historian en français},
	author = {Vidal-Gorène, Chahan},
	editor = {Levenson, Matthias Gille},
	collaborator = {Philip, Julien and Pinche, Ariane},
	month = jan,
	year = {2023},
}

@misc{pippi_how_2023,
	title = {How to {Choose} {Pretrained} {Handwriting} {Recognition} {Models} for {Single} {Writer} {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2305.02593},
	abstract = {Recent advancements in Deep Learning-based Handwritten Text Recognition (HTR) have led to models with remarkable performance on both modern and historical manuscripts in large benchmark datasets. Nonetheless, those models struggle to obtain the same performance when applied to manuscripts with peculiar characteristics, such as language, paper support, ink, and author handwriting. This issue is very relevant for valuable but small collections of documents preserved in historical archives, for which obtaining suﬃcient annotated training data is costly or, in some cases, unfeasible. To overcome this challenge, a possible solution is to pretrain HTR models on large datasets and then ﬁne-tune them on small single-author collections. In this paper, we take into account large, real benchmark datasets and synthetic ones obtained with a styled Handwritten Text Generation model. Through extensive experimental analysis, also considering the amount of ﬁne-tuning lines, we give a quantitative indication of the most relevant characteristics of such data for obtaining an HTR model able to eﬀectively transcribe manuscripts in small collections with as little as ﬁve real ﬁne-tuning lines.},
	language = {en},
	urldate = {2024-08-24},
	publisher = {arXiv},
	author = {Pippi, Vittorio and Cascianelli, Silvia and Kermorvant, Christopher and Cucchiara, Rita},
	month = may,
	year = {2023},
	note = {arXiv:2305.02593 [cs]},
	keywords = {Computer Science - Digital Libraries, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at ICDAR2023},
	file = {Pippi et al. - 2023 - How to Choose Pretrained Handwriting Recognition M.pdf:/Users/mpappia/Zotero/storage/KQ9GE33Y/Pippi et al. - 2023 - How to Choose Pretrained Handwriting Recognition M.pdf:application/pdf},
}

@misc{kohut_finetuning_2023,
	title = {Finetuning {Is} a {Surprisingly} {Effective} {Domain} {Adaptation} {Baseline} in {Handwriting} {Recognition}},
	url = {http://arxiv.org/abs/2302.06308},
	abstract = {In many machine learning tasks, a large general dataset and a small specialized dataset are available. In such situations, various domain adaptation methods can be used to adapt a general model to the target dataset. We show that in the case of neural networks trained for handwriting recognition using CTC, simple ﬁnetuning with data augmentation works surprisingly well in such scenarios and that it is resistant to overﬁtting even for very small target domain datasets. We evaluated the behavior of ﬁnetuning with respect to augmentation, training data size, and quality of the pre-trained network, both in writer-dependent and writer-independent settings. On a large real-world dataset, ﬁnetuning provided average relative CER improvement of 25 \% with 16 text lines for new writers and 50 \% for 256 text lines.},
	language = {en},
	urldate = {2024-08-24},
	publisher = {arXiv},
	author = {Kohút, Jan and Hradiš, Michal},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06308 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to ICDAR2023 conference},
	file = {Kohút et Hradiš - 2023 - Finetuning Is a Surprisingly Effective Domain Adap.pdf:/Users/mpappia/Zotero/storage/ILIG43ZN/Kohút et Hradiš - 2023 - Finetuning Is a Surprisingly Effective Domain Adap.pdf:application/pdf},
}
